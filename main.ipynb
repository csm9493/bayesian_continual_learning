{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from scipy.misc import imread\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from core import networks\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameters for training\n",
    "mini_batch_size = 128\n",
    "lambda_ = 1e-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataload (MNIST)\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('mnist-data/', train=True, download=True,\n",
    "                       transform=transforms.Compose([transforms.ToTensor(),])),\n",
    "        batch_size=mini_batch_size, shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('mnist-data/', train=False, transform=transforms.Compose([transforms.ToTensor(),])\n",
    "                       ),\n",
    "        batch_size=mini_batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#custom regularization\n",
    "\n",
    "import torch.nn as nn\n",
    "from core.networks import BayesianNetwork\n",
    "def custom_regularization(saver_net, trainer_net,mini_batch_size, lambda_, loss=None):\n",
    "    \n",
    "    mean_reg = 0\n",
    "    sigma_reg = 0\n",
    "    \n",
    "    #net1, net2에서 각 레이어에 있는 mean, sigma를 이용하여 regularization 구현\n",
    "\n",
    "    #각 모델에 module 접근\n",
    "    for saver, trainer in zip(saver_net.modules(),trainer_net.modules()):\n",
    "        \n",
    "        #만약 BayesianNetwork 이면\n",
    "        if isinstance(saver,BayesianNetwork) and  isinstance(trainer,BayesianNetwork):\n",
    "            \n",
    "            #Network 내부의 layer에 순차적으로 접근\n",
    "            for saver_layer, trainer_layer in zip(saver.layer_arr, trainer.layer_arr):\n",
    "            \n",
    "            # calculate mean regularization\n",
    "            \n",
    "                mean_reg += lambda_*(torch.div(trainer_layer.weight_mu, saver_layer.weight_sigma)-torch.div(trainer_layer.weight_mu, trainer_layer.weight_sigma)).norm(2)\n",
    "                \n",
    "                \n",
    "            # calculate sigma_reg regularization\n",
    "            \n",
    "                sigma_reg += torch.sum(torch.div(trainer_layer.weight_sigma, saver_layer.weight_sigma) - torch.log(torch.div(trainer_layer.weight_sigma, saver_layer.weight_sigma)))\n",
    "            \n",
    "            sigma_reg = sigma_reg/(mini_batch_size*2)\n",
    "            mean_reg = mean_reg/(mini_batch_size*2)\n",
    "                \n",
    "#             print (mean_reg, sigma_reg) # regularization value 확인\n",
    "    print ('loss')\n",
    "    print (loss)\n",
    "    print ()\n",
    "#     print ('mean_reg')\n",
    "#     print (mean_reg)\n",
    "#     print ()\n",
    "#     print ('sigma_reg')\n",
    "#     print (sigma_reg)\n",
    "#     print ()\n",
    "    \n",
    "    loss = loss + mean_reg + sigma_reg \n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(saver_net,trainer_net, optimizer, epoch, mini_batch_size, lambda_, DEVICE):\n",
    "    trainer_net.train()\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "        trainer_net.zero_grad()\n",
    "        loss = trainer_net.sample_elbo(data, target, mini_batch_size, DEVICE) #홍준 코딩\n",
    "        loss = custom_regularization(saver_net, trainer_net, mini_batch_size, lambda_, loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch/nn/_reduction.py:49: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss\n",
      "tensor(78724608., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(56832376., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(73653344., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(62907692., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(55189896., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(72313552., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(76388656., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(81100216., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(70801864., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(64384516., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(57632140., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(63240252., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(61461864., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(74705536., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(76205008., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(68832904., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(64551436., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(83990720., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(76858488., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(1.0607e+08, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(69569952., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(93270520., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(67460336., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(69632616., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(62001980., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(63272176., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(74605712., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(69367576., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(60720784., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(74311312., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(67738032., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(61216328., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(62910424., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(97754200., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(86390392., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(73355552., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(83954624., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(65969012., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(82713296., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(60512580., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(71281744., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(51997700., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(61766400., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(84606576., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(62341552., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(92102768., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(72962456., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(51038512., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(68217352., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(60734792., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(84579440., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(75292592., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(60767428., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(51290628., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(67373320., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(61154768., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(59665760., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(63349712., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(67980904., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(55095252., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(65309568., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(64370936., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(50973388., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(62692112., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(45135372., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(67117448., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(71701464., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(76501576., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(54076536., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(51285652., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(51733632., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(66652652., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(55995072., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(54487332., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(75454392., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(1.0130e+08, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(81394768., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(76149248., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(74052640., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(68773256., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(69056384., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(60942124., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(60360020., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(69946416., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(68613792., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(67046280., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(61781648., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(71157784., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(64036876., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(61196448., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(54592592., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(52726064., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(53097872., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(67052524., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(81458720., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(75541600., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(76222320., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(66640188., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(61096068., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(45767864., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(59507948., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(58685924., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(66535984., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(59544432., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(57207044., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(82959984., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(58615484., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(78930512., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(55161812., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(69187672., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(79452784., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(62089552., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(76486384., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(67490600., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(77335168., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(61808872., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(65459260., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(64352664., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(65417696., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(51557256., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(62861192., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(68370256., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss\n",
      "tensor(48683396., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(53821572., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(59588044., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(68655304., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(62109884., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(56865952., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(62971860., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(72722496., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(52254780., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(57335108., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(63181508., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(71447992., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(76357056., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(62193068., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(58244264., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(65827948., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(50137800., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(48209076., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(56137948., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(55735580., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(63120944., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(69307632., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(52930428., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(50357980., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(56130784., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(59871784., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(64616952., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(56737120., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(64176752., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(53969340., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(51418988., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(51846704., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(75499752., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(59311968., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(59124628., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(56235152., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(55149588., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(61152232., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(81369160., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(62347772., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(51342276., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(54646308., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(56915816., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(55507332., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(62807108., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(54385004., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(58154588., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(75280408., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(54489012., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(72620048., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(62394272., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(67799360., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(46259932., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(80982376., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(72877848., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(63019224., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(81916112., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(51080076., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(56305728., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(73015040., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(77077792., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(71674368., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(77052008., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(65262908., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(59951416., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(54504884., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(72514864., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(64922216., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(65131344., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(55626312., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(62945488., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(58529772., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(66420400., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(53716048., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(50245588., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(48082208., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(53817604., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(62622704., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(58945668., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(71515096., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(57693932., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(61549484., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(55363636., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(49376120., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(58190320., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(55264596., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(57603064., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(50608940., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(61907508., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(55170956., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(71585920., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(64262764., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(54849868., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(55872764., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(57160756., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(48972232., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(54466200., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(58978348., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(61627136., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(65901556., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(61079472., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(60501124., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(49493600., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(67054580., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(59040940., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(75727536., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(68364192., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(62180104., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(68828488., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(63028296., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(56462284., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(53278136., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(68468824., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(55322844., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(50069452., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(58218776., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(47911464., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(54777956., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(56163188., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(55675712., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(56407384., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(49414364., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(55251496., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(56348416., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss\n",
      "tensor(51535636., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(51365088., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(46112276., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(48226632., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(61357336., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(53586588., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(55524492., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(58016920., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(60046092., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(73190216., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(58896972., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(51654212., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(45099708., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(46627760., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(65525828., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(57266256., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(52228248., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(61982044., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(56385220., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(60589324., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(57992492., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(51541976., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(54683608., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(78002200., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(57113016., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(48152636., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(61430436., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(55132952., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(61248768., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(58766164., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(46960184., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(63967172., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(47488300., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(52094152., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(62141360., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(76097696., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(54663908., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(45989772., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(64400560., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(65361080., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(69418280., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(56488452., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(66618744., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(87900560., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(48395240., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(48053764., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(55888548., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(45868648., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(56163872., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(61093668., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(56085064., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(54277128., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(39311856., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(61435792., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(45539612., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(42744464., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(54716620., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(64361592., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(74350832., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(73517944., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(51281740., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(65094176., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(68440256., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(52489868., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(62695924., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(53375732., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(64311076., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(59539992., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(58094468., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(48144552., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(48149120., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(59522168., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(57237484., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(57172256., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(53884116., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(57540148., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(62611148., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(56819264., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(49875164., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(51683324., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(54811696., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(52885852., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(59050920., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(61607008., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(68856624., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(54943660., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(56965556., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(58123880., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(64180932., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(52032344., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(62332680., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(54891280., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(45528192., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(54797292., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(50626000., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(50957536., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(47790416., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(59620484., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(59885856., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(65664388., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(52304520., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(54839188., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(57441352., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(49455500., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(59847480., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(42986120., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(56489448., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(68152408., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(63336808., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(58786496., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(52205644., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(76007880., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(63873488., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(74241016., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(49169368., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(57208472., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(50490144., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(48694508., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(54910516., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(51005872., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(53103532., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(54539120., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss\n",
      "tensor(56762424., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(39694148., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(53770136., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(40915828., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(45701292., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(56554888., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(51008556., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(47464420., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(44651372., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(49872744., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(42926532., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(62048024., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(48587028., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(48610984., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(51088764., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(57831608., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(54936988., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(45464024., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(55622392., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(59265016., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(45959544., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(51241128., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(51754188., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(42432340., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(57241528., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(62942932., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(43403204., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(52163300., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(64781500., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(52248764., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(48991044., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(46623788., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(48148384., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(53369152., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(57420088., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(62230920., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(51945692., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(57776480., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(44388844., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(69653168., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(43947192., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(43013136., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(39565936., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(55459336., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(49493148., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(50957784., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(40644720., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(58274344., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(49812380., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(58299128., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(70695760., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(46491844., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(60105104., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(50062728., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(62638216., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(53641144., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(63050440., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(47759692., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(60024416., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(68094712., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(48007040., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(63067040., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(39869216., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(44815928., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(49563340., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(46661948., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(35192656., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(40724940., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(51884728., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(50798620., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(54467664., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(54051972., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(53625212., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(47824576., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(40771408., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(42640032., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(48878792., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(59183020., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(53720956., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(51842632., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(48027532., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(47238836., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(62061768., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(46907200., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(45285496., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(53043460., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(48831892., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(57487696., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(77263152., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(48927232., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(49152588., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(57790676., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(44454860., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(44818012., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(50888872., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(53526900., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(55884192., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(47443932., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(44081672., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n",
      "loss\n",
      "tensor(45051248., device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The expanded size of the tensor (128) must match the existing size (96) at non-singleton dimension 0.  Target sizes: [128, 10].  Tensor sizes: [96, 10]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-23b81911da57>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;31m#1. trainet_net training 하는데 regularization을 위해서 saver_net의 정보 이용\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msaver_net\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer_net\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmini_batch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;31m#2. 1 batch가 끝나면 saver_net에 trainet_net을 복사 (weight = mean, sigma)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-f3fa1e4fe94f>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(saver_net, trainer_net, optimizer, epoch, mini_batch_size, lambda_, DEVICE)\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mtrainer_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_elbo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmini_batch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#홍준 코딩\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcustom_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msaver_net\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer_net\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmini_batch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/RL/researchers/sungmin/bayesianDeepLearning/bayesian_continual_learning_research/core/networks.py\u001b[0m in \u001b[0;36msample_elbo\u001b[0;34m(self, data, target, BATCH_SIZE, DEVICE, samples)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m             \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0mnegative_log_likelihood\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize_average\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The expanded size of the tensor (128) must match the existing size (96) at non-singleton dimension 0.  Target sizes: [128, 10].  Tensor sizes: [96, 10]"
     ]
    }
   ],
   "source": [
    "# GPU 설정\n",
    "device_num = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#Model Initialization\n",
    "#Saver_Net : mu = 0, sigma = log(1+exp(1))\n",
    "#trainer_ner : mu = [-5,5], sigma = log(1+exp([-5,+5]))\n",
    "saver_net = networks.BayesianNetwork(init_type = 'zero', DEVICE = device_num).to(device_num)\n",
    "trainer_net = networks.BayesianNetwork(init_type = 'random', DEVICE = device_num).to(device_num)\n",
    "\n",
    "optimizer = optim.Adam(saver_net.parameters())\n",
    "optimizer = optim.Adam(trainer_net.parameters())\n",
    "\n",
    "for epoch in range(10):\n",
    "\n",
    "    #0. trainet_net variance init\n",
    "    \n",
    "    trainer_net.variance_init() #trainer net의 variance크게 init\n",
    "    trainer_net = trainer_net.to(device_num)\n",
    "    \n",
    "    #1. trainet_net training 하는데 regularization을 위해서 saver_net의 정보 이용\n",
    "    \n",
    "    train(saver_net, trainer_net, optimizer, epoch, mini_batch_size, lambda_, device_num)\n",
    "\n",
    "    #2. 1 batch가 끝나면 saver_net에 trainet_net을 복사 (weight = mean, sigma)\n",
    "    \n",
    "    saver_net = copy.deepcopy(trainer_net)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
