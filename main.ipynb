{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from scipy.misc import imread\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from core import networks\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameters for training\n",
    "mini_batch_size = 128\n",
    "lambda_ = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataload (MNIST)\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('mnist-data/', train=True, download=True,\n",
    "                       transform=transforms.Compose([transforms.ToTensor(),])),\n",
    "        batch_size=mini_batch_size, shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('mnist-data/', train=False, transform=transforms.Compose([transforms.ToTensor(),])\n",
    "                       ),\n",
    "        batch_size=mini_batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#custom regularization\n",
    "\n",
    "import torch.nn as nn\n",
    "from core.networks import BayesianNetwork\n",
    "def custom_regularization(saver_net, trainer_net,mini_batch_size, lambda_, loss=None):\n",
    "    \n",
    "    mean_reg = 0\n",
    "    sigma_reg = 0\n",
    "    \n",
    "    #net1, net2에서 각 레이어에 있는 mean, sigma를 이용하여 regularization 구현\n",
    "\n",
    "    #각 모델에 module 접근\n",
    "    for saver, trainer in zip(saver_net.modules(),trainer_net.modules()):\n",
    "        \n",
    "        #만약 BayesianNetwork 이면\n",
    "        if isinstance(saver,BayesianNetwork) and  isinstance(trainer,BayesianNetwork):\n",
    "            \n",
    "            #Network 내부의 layer에 순차적으로 접근\n",
    "            for saver_layer, trainer_layer in zip(saver.layer_arr, trainer.layer_arr):\n",
    "            \n",
    "            # calculate mean regularization\n",
    "            \n",
    "                mean_reg += lambda_*(torch.div(trainer_layer.weight_mu, saver_layer.weight_sigma)-torch.div(trainer_layer.weight_mu, trainer_layer.weight_sigma)).norm(2)\n",
    "                mean_reg = mean_reg/(mini_batch_size*2)\n",
    "                \n",
    "            # calculate sigma_reg regularization\n",
    "            \n",
    "                sigma_reg += torch.sum(torch.div(trainer_layer.weight_sigma, saver_layer.weight_sigma) - torch.log(torch.div(trainer_layer.weight_sigma, saver_layer.weight_sigma)))\n",
    "                sigma_reg = sigma_reg/(mini_batch_size*2)\n",
    "                \n",
    "                print (mean_reg, sigma_reg) # regularization value 확인\n",
    "                \n",
    "    loss = loss + mean_reg + sigma_reg \n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(saver_net,trainer_net, optimizer, epoch, mini_batch_size, lambda_):\n",
    "    trainer_net.train()\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "        trainer_net.zero_grad()\n",
    "        #         loss = net.sample_elbo(data, target) #홍준 코딩\n",
    "        loss = custom_regularization(saver_net, trainer_net, mini_batch_size, lambda_)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.0934, device='cuda:0', grad_fn=<DivBackward0>) tensor(2722.7454, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(1.4884, device='cuda:0', grad_fn=<DivBackward0>) tensor(1394.8323, device='cuda:0', grad_fn=<DivBackward0>)\n",
      "tensor(0.2388, device='cuda:0', grad_fn=<DivBackward0>) tensor(39.9875, device='cuda:0', grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "add(): argument 'other' (position 1) must be Tensor, not NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-641ebc737f4a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;31m#1. trainet_net training 하는데 regularization을 위해서 saver_net의 정보 이용\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msaver_net\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer_net\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmini_batch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;31m#2. 1 batch가 끝나면 saver_net에 trainet_net을 복사 (weight = mean, sigma)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-6ad0a5016faa>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(saver_net, trainer_net, optimizer, epoch, mini_batch_size, lambda_)\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mtrainer_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;31m#         loss = net.sample_elbo(data, target) #홍준 코딩\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcustom_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msaver_net\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer_net\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmini_batch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambda_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-b1ec34b63405>\u001b[0m in \u001b[0;36mcustom_regularization\u001b[0;34m(saver_net, trainer_net, mini_batch_size, lambda_, loss)\u001b[0m\n\u001b[1;32m     31\u001b[0m                 \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmean_reg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msigma_reg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# regularization value 확인\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmean_reg\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msigma_reg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: add(): argument 'other' (position 1) must be Tensor, not NoneType"
     ]
    }
   ],
   "source": [
    "# GPU 설정\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#Model Initialization\n",
    "#Saver_Net : mu = 0, sigma = log(1+exp(1))\n",
    "#trainer_ner : mu = [-5,5], sigma = log(1+exp([-5,+5]))\n",
    "saver_net = networks.BayesianNetwork(init_type = 'zero').to(DEVICE)\n",
    "trainer_net = networks.BayesianNetwork(init_type = 'random').to(DEVICE)\n",
    "\n",
    "optimizer = optim.Adam(saver_net.parameters())\n",
    "optimizer = optim.Adam(trainer_net.parameters())\n",
    "\n",
    "for epoch in range(10):\n",
    "\n",
    "    #0. trainet_net variance init\n",
    "    \n",
    "    trainer_net.variance_init() #trainer net의 variance크게 init\n",
    "    trainer_net = trainer_net.to(DEVICE)\n",
    "    \n",
    "    #1. trainet_net training 하는데 regularization을 위해서 saver_net의 정보 이용\n",
    "    \n",
    "    train(saver_net, trainer_net, optimizer, epoch, mini_batch_size, lambda_)\n",
    "\n",
    "    #2. 1 batch가 끝나면 saver_net에 trainet_net을 복사 (weight = mean, sigma)\n",
    "    \n",
    "    saver_net = copy.deepcopy(trainer_net)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
