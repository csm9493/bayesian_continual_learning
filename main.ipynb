{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from scipy.misc import imread\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from core import networks\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameters for training\n",
    "mini_batch_size = 128\n",
    "lambda_ = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataload (MNIST)\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('mnist-data/', train=True, download=True,\n",
    "                       transform=transforms.Compose([transforms.ToTensor(),])),\n",
    "        batch_size=mini_batch_size, shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('mnist-data/', train=False, transform=transforms.Compose([transforms.ToTensor(),])\n",
    "                       ),\n",
    "        batch_size=mini_batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#custom regularization\n",
    "\n",
    "import torch.nn as nn\n",
    "from core.networks import BayesianNetwork\n",
    "def custom_regularization(saver_net, trainer_net,mini_batch_size, lambda_, loss=None):\n",
    "    \n",
    "    mean_reg = 0\n",
    "    sigma_reg = 0\n",
    "    \n",
    "    #net1, net2에서 각 레이어에 있는 mean, sigma를 이용하여 regularization 구현\n",
    "\n",
    "    #각 모델에 module 접근\n",
    "    for saver, trainer in zip(saver_net.modules(),trainer_net.modules()):\n",
    "        \n",
    "        #만약 BayesianNetwork 이면\n",
    "        if isinstance(saver,BayesianNetwork) and isinstance(trainer,BayesianNetwork):\n",
    "            \n",
    "            i = 0\n",
    "            \n",
    "            #Network 내부의 layer에 순차적으로 접근\n",
    "            for saver_layer, trainer_layer in zip(saver.layer_arr, trainer.layer_arr):\n",
    "            \n",
    "            # calculate mean regularization\n",
    "\n",
    "                trainer_mu = trainer_layer.weight_mu\n",
    "                saver_mu = saver_layer.weight_mu\n",
    "                \n",
    "                trainer_sigma = torch.log1p(torch.exp(trainer_layer.weight_rho))\n",
    "                saver_sigma = torch.log1p(torch.exp(saver_layer.weight_rho))\n",
    "                \n",
    "                \n",
    "                \n",
    "                #mean_reg += lambda_*(torch.div(trainer_layer.weight_mu, saver_layer.weight_rho)-torch.div(trainer_layer.weight_mu, trainer_layer.weight_rho)).norm(2)\n",
    "                mean_reg += lambda_*(torch.div(trainer_mu, saver_sigma)-torch.div(saver_mu, saver_sigma)).norm(2)\n",
    "    \n",
    "            # calculate sigma_reg regularization\n",
    "            \n",
    "                #sigma_reg += torch.sum(torch.div(trainer_layer.weight_rho, saver_layer.weight_rho) - torch.log(torch.div(trainer_layer.weight_rho, saver_layer.weight_rho)))\n",
    "                sigma_reg += torch.sum(torch.div(trainer_sigma*trainer_sigma, saver_sigma*saver_sigma) - torch.log(torch.div(trainer_sigma*trainer_sigma, saver_sigma*saver_sigma)))\n",
    "\n",
    "            sigma_reg = sigma_reg/(mini_batch_size*2)\n",
    "            mean_reg = mean_reg/(mini_batch_size*2)\n",
    "            loss = loss/mini_batch_size\n",
    "                \n",
    "#             print (mean_reg, sigma_reg) # regularization value 확인\n",
    "\n",
    "    loss = loss + mean_reg + sigma_reg \n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(saver_net,trainer_net, optimizer, epoch, mini_batch_size, lambda_, DEVICE):\n",
    "    trainer_net.train()\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "        if data.shape[0] == mini_batch_size:\n",
    "            #trainer_net.zero_grad()\n",
    "            loss = trainer_net.sample_elbo(data, target, mini_batch_size, DEVICE)\n",
    "            #loss = custom_regularization(saver_net, trainer_net, mini_batch_size, lambda_, loss)\n",
    "            loss.backward()\n",
    "            #print(trainer_net.l2.weight.rho.grad)\n",
    "            #print(trainer_net.l.weight.rho.grad)\n",
    "            #print(trainer_net.l1.weight.rho.grad.norm(2))\n",
    "            \n",
    "            optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data, sample=True)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True) # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch/nn/_reduction.py:49: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 119.6466, Accuracy: 8734/10000 (87%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 86.9003, Accuracy: 9078/10000 (91%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 53.7971, Accuracy: 9173/10000 (92%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 33.2610, Accuracy: 9230/10000 (92%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 22.8122, Accuracy: 9127/10000 (91%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 17.7324, Accuracy: 8871/10000 (89%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 16.2816, Accuracy: 8564/10000 (86%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 14.3006, Accuracy: 8375/10000 (84%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 12.2172, Accuracy: 8222/10000 (82%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 11.0288, Accuracy: 8402/10000 (84%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# GPU 설정\n",
    "device_num = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#Model Initialization\n",
    "#Saver_Net : mu = 0, sigma = log(1+exp(1))\n",
    "#trainer_ner : mu = [-5,5], sigma = log(1+exp([+1,+1]))\n",
    "saver_net = networks.BayesianNetwork(init_type = 'zero', DEVICE = device_num).to(device_num)\n",
    "trainer_net = networks.BayesianNetwork(init_type = 'random', DEVICE = device_num).to(device_num)\n",
    "\n",
    "optimizer = optim.Adam(saver_net.parameters())\n",
    "optimizer = optim.Adam(trainer_net.parameters())\n",
    "\n",
    "for epoch in range(10):\n",
    "\n",
    "    #0. trainet_net variance init\n",
    "    \n",
    "    trainer_net.variance_init() #trainer net의 variance크게 init\n",
    "    trainer_net = trainer_net.to(device_num)\n",
    "    \n",
    "    #1. trainer_net training 하는데 regularization을 위해서 saver_net의 정보 이용\n",
    "    \n",
    "    train(saver_net, trainer_net, optimizer, epoch, mini_batch_size, lambda_, device_num)\n",
    "\n",
    "    #2. 1 batch가 끝나면 saver_net에 trainet_net을 복사 (weight = mean, sigma)\n",
    "    \n",
    "    saver_net = copy.deepcopy(trainer_net)\n",
    "    \n",
    "    test(trainer_net, device_num, test_loader)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
